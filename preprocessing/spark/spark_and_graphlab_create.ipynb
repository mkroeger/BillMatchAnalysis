{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Spark notebooks\n",
    "\n",
    "Prerequisites for running the Spark notebook is git and maven installed on your laptop.\n",
    "\n",
    "We will install the Apache Spark release 1.5 from the source code:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/apache/spark\n",
    "cd spark\n",
    "build/mvn -DskipTests clean package\n",
    "```\n",
    "\n",
    "To run th eSpark enabled notebook re-launch your iPython notebook as follows:\n",
    "\n",
    "```bash\n",
    "IPYTHON_OPTS=\"notebook\" ~/tmp_spark/spark/bin/pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GraphLab Create with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is to learn a topic model using Wikipedia data, to see what topics are most represented in Wikipedia. The parts required for this project are:\n",
    "1. [Set up environment](#Step-1:-Set-up-environment)\n",
    "1. [Turn Raw Wikipedia text into Bag of Words, Using Spark](#Step-2:-Turn-Raw-Wikipedia-text-into-Bag-of-Words,-Using-Spark)\n",
    "1. [Ingest Spark RDD as SFrame](#Step-3:-Ingest-Spark-RDD-as-SFrame)\n",
    "1. [Learn Topic Model](#Step-4:-Learn-Topic-Model)\n",
    "1. [Explore topics](#Step-5:-Explore-the-Topics)\n",
    "1. [Save Results to Spark RDD](#Step-6:-Save-Results-to-Spark-RDD)\n",
    "\n",
    "By using PySpark and GraphLab Create together this notebook shows how easy it is to use both systems together.\n",
    "\n",
    "#### Note: This notebook requires GraphLab Create 1.3 and Spark 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphLab Create ships with a Spark Integration JAR, which is required in order to use PySpark with GraphLab Create. This JAR needs to be added to the Spark CLASSPATH. The following shell script will add the JAR to the ```spark-defaults.conf``` file. If running locally, you may want to configure the ```spark.driver.memory``` parameter to a larger value so the JVM doesn't run out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphLab Spark Integration JAR already added to Spark Configuration, doing nothing.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#export HADOOP_CONF_DIR=$HOME/hadoop-2.x/etc/hadoop\n",
    "export SPARK_HOME=/Users/asvyatko/tmp_spark/spark #$HOME/spark-1.1.1-bin-hadoop2.4\n",
    "export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "\n",
    "if [ ! -f $SPARK_HOME/conf/spark-defaults.conf ]\n",
    "then\n",
    "    echo \"Copying spark-defaults.conf.template to spark-defaults.conf\"\n",
    "    echo \"If running Spark with 'local' context, you may want to increase the spark.driver.memory parameter\"\n",
    "    echo \"\"\n",
    "    cp $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_HOME/conf/spark-defaults.conf\n",
    "fi\n",
    "\n",
    "already_exists=$(grep -c graphlab-create-spark-integration.jar $SPARK_HOME/conf/spark-defaults.conf)\n",
    "if [ $already_exists -eq 0 ]\n",
    "then\n",
    "    echo \"spark.driver.extraClassPath `python -c 'import graphlab as gl; print gl.get_spark_integration_jar_path();'` \" >> $SPARK_HOME/conf/spark-defaults.conf\n",
    "    echo \"Added GraphLab Spark Integration JAR to $SPARK_HOME/conf/spark-defaults.conf\"\n",
    "else\n",
    "    echo \"GraphLab Spark Integration JAR already added to Spark Configuration, doing nothing.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Turn Raw Wikipedia text into Bag of Words, Using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Unable to write current GraphLab Create license to /Users/asvyatko/.graphlab/config. Ensure that this user account has write permission to /Users/asvyatko/.graphlab/config to save the license for offline use.\n",
      "[INFO] This trial license of GraphLab Create is assigned to alexeys@princeton.edu and will expire on August 17, 2015. Please contact trial@dato.com for licensing options or to request a free non-commercial license for personal or academic use.\n",
      "\n",
      "[INFO] Start server at: ipc:///tmp/graphlab_server-5208 - Server binary: /Library/Python/2.7/site-packages/graphlab/unity_server - Server log: /tmp/graphlab_server_1437753052.log\n",
      "[INFO] GraphLab Server Version: 1.5.1\n",
      "[WARNING] Unable to create session in specified location: '/Users/asvyatko/.graphlab/artifacts'. Using: '/var/tmp/graphlab-asvyatko/5208/tmp_session_d1583a50-5d14-4000-a6e0-127bb3c4d00f'\n"
     ]
    }
   ],
   "source": [
    "import graphlab as gl\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the SparkContext setup, let's download the Wikipedia data as an RDD. For this notebook we will only use a subset of the data, but there is code below to use the full dataset (which is about ~5GB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(os.path.join(save_path, local_filename), 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "                f.flush()\n",
    "    return local_filename\n",
    "\n",
    "# File to download\n",
    "file_list = [16] \n",
    "\n",
    "# If you want to use this entire Wikipedia dataset, uncomment the following line.\n",
    "# This will download ~5GB of data split over 36 files.\n",
    "# file_list = range(37)\n",
    "\n",
    "# Download location for Wikipedia data\n",
    "save_path = '/Users/asvyatko/Desktop/dato_conf_2015-deep_learning-notebooks/wikipedia' #'/tmp/wikipedia'\n",
    "\n",
    "# Actually download the files, if the location doesn't exist yet.\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    for idx in file_list: \n",
    "        url = 'http://s3.amazonaws.com/dato-datasets/wikipedia/raw/w%d' % idx\n",
    "        print \"Downloading '%s', saving to: '%s'\" % (url, save_path)\n",
    "        download_file(url, save_path) # This will download 146MB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawRdd = sc.textFile('file:///%s/' % save_path).zipWithIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the rdd is defined, let's see the first few lines to confirm it is structured the way we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'alainconnes alain connes is one of the leading specialists on operator algebras  in his early work on von neumann algebras in the 1970s he succeeded in obtaining the almost complete classification of injective factors  following this he made contributions in operator ktheory and index theory which culminated in the baumconnes conjecture he also introduced cyclic cohomology in the early 1980s as a first step in the study of noncommutative differential geometry connes has applied his work in areas of mathematics and theoretical physics including number theory differential geometry and particle physics connes was awarded the fields medal in 1982 the crafoord prize in 2001 and the gold medal of the cnrs in 2004   he is a member of the french academy of sciences and several foreign academies and societies including the danish academy of sciences norwegian academy of sciences russian academy of sciences and us national academy of sciences ',\n",
       "  0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, it has a document on each line, with a subsequent index value. Since we want to split documents by space, it is important to remove any extra spaces that exist. When examining the document closely we see there are extra spaces, so let's clean those up and split each document by space. Also, let's put the index for the document as the first entry, so we have an 'id' key and then the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [u'alainconnes',\n",
       "   u'alain',\n",
       "   u'connes',\n",
       "   u'is',\n",
       "   u'one',\n",
       "   u'of',\n",
       "   u'the',\n",
       "   u'leading',\n",
       "   u'specialists',\n",
       "   u'on',\n",
       "   u'operator',\n",
       "   u'algebras',\n",
       "   u'in',\n",
       "   u'his',\n",
       "   u'early',\n",
       "   u'work',\n",
       "   u'on',\n",
       "   u'von',\n",
       "   u'neumann',\n",
       "   u'algebras',\n",
       "   u'in',\n",
       "   u'the',\n",
       "   u'1970s',\n",
       "   u'he',\n",
       "   u'succeeded',\n",
       "   u'in',\n",
       "   u'obtaining',\n",
       "   u'the',\n",
       "   u'almost',\n",
       "   u'complete',\n",
       "   u'classification',\n",
       "   u'of',\n",
       "   u'injective',\n",
       "   u'factors',\n",
       "   u'following',\n",
       "   u'this',\n",
       "   u'he',\n",
       "   u'made',\n",
       "   u'contributions',\n",
       "   u'in',\n",
       "   u'operator',\n",
       "   u'ktheory',\n",
       "   u'and',\n",
       "   u'index',\n",
       "   u'theory',\n",
       "   u'which',\n",
       "   u'culminated',\n",
       "   u'in',\n",
       "   u'the',\n",
       "   u'baumconnes',\n",
       "   u'conjecture',\n",
       "   u'he',\n",
       "   u'also',\n",
       "   u'introduced',\n",
       "   u'cyclic',\n",
       "   u'cohomology',\n",
       "   u'in',\n",
       "   u'the',\n",
       "   u'early',\n",
       "   u'1980s',\n",
       "   u'as',\n",
       "   u'a',\n",
       "   u'first',\n",
       "   u'step',\n",
       "   u'in',\n",
       "   u'the',\n",
       "   u'study',\n",
       "   u'of',\n",
       "   u'noncommutative',\n",
       "   u'differential',\n",
       "   u'geometry',\n",
       "   u'connes',\n",
       "   u'has',\n",
       "   u'applied',\n",
       "   u'his',\n",
       "   u'work',\n",
       "   u'in',\n",
       "   u'areas',\n",
       "   u'of',\n",
       "   u'mathematics',\n",
       "   u'and',\n",
       "   u'theoretical',\n",
       "   u'physics',\n",
       "   u'including',\n",
       "   u'number',\n",
       "   u'theory',\n",
       "   u'differential',\n",
       "   u'geometry',\n",
       "   u'and',\n",
       "   u'particle',\n",
       "   u'physics',\n",
       "   u'connes',\n",
       "   u'was',\n",
       "   u'awarded',\n",
       "   u'the',\n",
       "   u'fields',\n",
       "   u'medal',\n",
       "   u'in',\n",
       "   u'1982',\n",
       "   u'the',\n",
       "   u'crafoord',\n",
       "   u'prize',\n",
       "   u'in',\n",
       "   u'2001',\n",
       "   u'and',\n",
       "   u'the',\n",
       "   u'gold',\n",
       "   u'medal',\n",
       "   u'of',\n",
       "   u'the',\n",
       "   u'cnrs',\n",
       "   u'in',\n",
       "   u'2004',\n",
       "   u'he',\n",
       "   u'is',\n",
       "   u'a',\n",
       "   u'member',\n",
       "   u'of',\n",
       "   u'the',\n",
       "   u'french',\n",
       "   u'academy',\n",
       "   u'of',\n",
       "   u'sciences',\n",
       "   u'and',\n",
       "   u'several',\n",
       "   u'foreign',\n",
       "   u'academies',\n",
       "   u'and',\n",
       "   u'societies',\n",
       "   u'including',\n",
       "   u'the',\n",
       "   u'danish',\n",
       "   u'academy',\n",
       "   u'of',\n",
       "   u'sciences',\n",
       "   u'norwegian',\n",
       "   u'academy',\n",
       "   u'of',\n",
       "   u'sciences',\n",
       "   u'russian',\n",
       "   u'academy',\n",
       "   u'of',\n",
       "   u'sciences',\n",
       "   u'and',\n",
       "   u'us',\n",
       "   u'national',\n",
       "   u'academy',\n",
       "   u'of',\n",
       "   u'sciences'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace multiple spaces with a single space using re.sub, then trim whitespace and split on single space.\n",
    "import re\n",
    "splitRdd = rawRdd.map(lambda (a,b): (b, re.sub(\"[ ]+\", \" \", a).strip().split(\" \")))\n",
    "splitRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each document is a tuple of (index, list of words). Let's transform that into a list of (index, word) tuples instead. We will use flatMap for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, u'alainconnes')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipRdd = splitRdd.flatMap(lambda (a,b): zip([a] * len(b),b))\n",
    "zipRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have things formatted the way we want, let's start aggregating to generate word counts per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordRdd = zipRdd.map(lambda composite_word: (composite_word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "wordRdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's create a dictionary with word as the key and count as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bagRdd = wordRdd.map(lambda (a,b):(a[0],(a[1],b))).groupByKey().map(lambda (a,b):(a,{word_count[0]:word_count[1] for word_count in b.data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Ingest Spark RDD as SFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the raw Wikipedia text converted into a bag-of-words using Spark, it is easy to ingest that into GraphLab Create as an SFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = gl.SFrame.from_rdd(bagRdd)\n",
    "data = data.unpack('X1')\n",
    "data.rename({'X1.0':'id','X1.1':'bag_of_words'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gl.canvas.set_target('ipynb')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the most frequent words in the bag of words, it is obvious that 'stop words' are most prevalent. Let's remove them with one line, using GraphLab Create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trim out stopwords\n",
    "data['bag_of_words'] = data['bag_of_words'].dict_trim_by_keys(gl.text_analytics.stopwords(), exclude=True)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now the most frequent words are no longer stop words. We are ready to train a Topic Model on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Learn Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have an SFrame, training a Topic Model is one line. We are saying we are looking for the model to learn five topics, and to train for ten iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If running on entire dataset, might want to increase num_topics and num_iterations\n",
    "model = gl.topic_model.create(data['bag_of_words'], num_topics=5, num_iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Explore the Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get topic ids predicted from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model.predict(data['bag_of_words'])\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that is just showing predicted topic_id. Instead, let's add a column with the topic_id we just predicted, and create that as our results SFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = gl.SFrame({'doc_id':data['id'], 'topic_id':pred, 'bag_of_words':data['bag_of_words']})\n",
    "results.swap_columns('doc_id', 'bag_of_words') # better SFrame formatting\n",
    "results.print_rows(max_column_width=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see which topic ids appear most frequently in this set of Wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gl.canvas.set_target('ipynb')\n",
    "results['topic_id'].show('Categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this tells us that topic ids 3 and 2 are more common in this dataset. Let's find out what words are associated with those topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.get_topics([3], output_type='topic_words').print_rows(max_column_width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.get_topics([2], output_type='topic_words').print_rows(max_column_width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. Wonder what this set of documents is about. Let's get the full list of topic words learned by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = model.get_topics()\n",
    "topics = topics.rename({'topic':'topic_id'})\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That SFrame is less useful, let's groupby all the same topic ids and create a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics.groupby(['topic_id'], {'topic_words':gl.aggregate.CONCAT(\"word\")}).print_rows(max_column_width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the appropriate format for recording the topics learned, by topic_id.\n",
    "\n",
    "Great, so now we have the results SFrame and the Topics SFrame, both of which can be saved back as Spark RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Save Results to Spark RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have all the results ready as two SFrames. The first has the bag-of-words with the predicted topic_id, and the second has the topic words for each topic_id. These are both tables we can save as Spark RDDs, so subsequent Spark programs can utilize the findings from the Topic Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to save the predictions as an RDD\n",
    "predictions_rdd = data.to_rdd(sc)\n",
    "predictions_rdd.saveAsTextFile('file:///tmp/predictions.rdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the topic_ids with their topic words\n",
    "topics_rdd = topics.to_rdd(sc)\n",
    "topics_rdd.saveAsTextFile('file:///tmp/topics.rdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! GraphLab Create works well with Apache Spark, allowing you to leverage what you've already built in Spark with GraphLab Create. No need to save to intermediary formats just to train ML models in GraphLab Create.\n",
    "\n",
    "For more information on using Apache Spark with GraphLab Create, check out the [User Guide section](http://dato.com/learn/userguide/#Spark_Integration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
